## Problem setup

**Goal of policy evaluation** - to evaluate how good(action-state q or value-state v) a policy is 

**Definition of return $G_t$** - 

$$ G_t = \sum_{i=0}^n \gamma^i R_{i+1}$$

**Definition of $V_\pi(s)$ and $Q_\pi(s, a)$** - 


$$ V_\pi = E_\pi[G_t | s_t = s ]$$
$$ Q_\pi = E_\pi[G_t | s_t = s, a_t = a ]$$



## Why model-free policy evaluation matters

Estimating the expected return of a particular policy if we don't have access to the true MDP models 

## Monte Carlo (MC) policy evaluation

**Core idea of MC estimation** - 

$$ V_\pi (s) = E_{\tau \approx \pi} [G_t | s_t = s] $$

Expectation over trajectories $\tau$ generated by following $\pi$. If trajectories are all finite, sample set of trajectories & average returns.  (Value = mean return) 

**First-visit vs every-visit MC** - 

Each occurrence of state s in an episode is called a **visit** to s.

The **first-visit MC method** estimates $v_\pi(s)$ as the average of the returns following first visits to s, whereas the **every-visit MC method** averages the returns following all visits to s. 

First visit MC method algorithm for estimating $v_\pi$ 

Initialize: $\pi$ (policy to be evaluated); V (an arbitrary state-value function) 

Repeat forever: 

> Generate an episode using $\pi$ 

> For each state s appearing in the episode: 

    >> G <- return following the first occurence of s

    >> Append G to Return(s) 

    >> V(s) <- average Return(s) 



Each average is itself an unbiased estimate, and the standard deviation of its error falls as $1/\sqrt(n)$ where n is the number of returns averaged. 

**Incremental MC update rule** - Update average return of a state using one new sample, without storing past samples. 

$$ V(s) = V(s) + \alpha [G - V(s)] $$ 

Here $\alpha$ is the step size, i.e. how much you trust the new sample. 

G is still the full return until the end of the episode. No bootstrapping or no guessing future values. 

**Limitations of Monte Carlo** - 

High variance - full returns accumulate randomness over many steps, leading to noisy estimates. 

Data inefficiency in long horizons - Long episodes require many samples before estimates stabilize. 

## Temporal Difference (TD) learning 

**Motivation for TD**- reduce variance and allow learning before episode termination by combining sampling with bootstrapping. 

**Bootstrapping concept** - updating a value estimate using another learned estimate instead of waiting for the true return. 

**TD(0) update rule** - updates the current state value using the immediate reward and estimated value of the next state. 

**TD error $\delta_t$** - TD error measures the difference between predicted value and the observed reward plus discounted next value. 

## TD vs MC comparison

**Bias vs variance tradeoff** - MC is unbiased but high variance. TD introduces bias but significantly reduces variance. 

**Sample efficiency** - TD typically needs fewer samples to reach reasonable accuracy.

**Update timing** - MC updates after episode completion. TD updates immediately after each transition. 

**Dependence on Markov property** - TD relies on Markov assumption. MC does not strictly require it. 

## Batch (offline) policy evaluation

**Definition of batch setting** - Learning is done from a fixed dataset with no new data collection. 

**Repeated replay of fixed dataset** - The same episodes are reused multiple times for updates. 

**Behavior of MC in batch** - Batch MC converges to the value function that minimizes mean squared error over observed returns. 

**Behavior of TD in batch** - Batch TD converges to the value function of the maximum likelihood estimated MDP.

## Certainty Equivalence policy evaluation

**MLE estimation of transition probabilities** - Transitions are estimated by counting observed state-action-next-state frequencies. 

**MLE estimation of rewards** - Rewards are estimated by averaging observed rewards for each state-action pair. 

**Solving estimated MDP via DP** - Dynamic programming is applied to the estimated model to compute values. 